{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5170db5",
   "metadata": {},
   "source": [
    "#    DESAFÍO 2\n",
    "##   Modelo de Regresión\n",
    "###  Grupo 4: Florencia Accardo, Juan Mutis, Joaquín Fernández, Rodrigo Arias, Ignacio Nasso "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0ec34",
   "metadata": {},
   "source": [
    "<a id=\"section_toc\"></a> \n",
    "## Indice\n",
    "\n",
    "<a href=\"#section_intro\">PARTE INICIAL. IMPORTACIÓN DE LIBRERÍAS. IMPORTACIÓN DE DATA FINAL DESAFÍO 1</a>\n",
    "\n",
    "<a href=\"#section_variables\">NUEVAS VARIABLES</a>\n",
    "\n",
    "<a href=\"#section_dummies\">GENERAMOS DUMMIES</a>\n",
    "\n",
    "<a href=\"#section_estandar\">ESTANDARIZAMOS VARIABLES NUMÉRICAS</a>\n",
    "\n",
    "<a href=\"#section_data\">PREPARAMOS DATA FRAME A INGRESAR EN LOS MODELOS</a>\n",
    "\n",
    "<a href=\"#section_reg_sts\">REGRESIÓN LINEAL CON STASTMODEL</a>\n",
    "\n",
    "<a href=\"#section_reg_skl\">REGRESIÓN LINEAL CON SKLEARN</a>\n",
    "\n",
    "<a href=\"#section_vif\">EVALUAMOS VIF</a>\n",
    "\n",
    "<a href=\"#section_lasso_cv\">REGULARIZAMOS CON LASSO</a>\n",
    "\n",
    "<a href=\"#section_ridge_cv\">REGULARIZAMOS CON RIDGE</a>\n",
    "\n",
    "<a href=\"#section_dec_tree\">PROBAMOS DECISION TREE</a>\n",
    "\n",
    "<a href=\"#section_ensemble\">PROBAMOS MODELOS DE ENSEMBLE</a>\n",
    "\n",
    "<a href=\"#section_gauss\">EVALUAMOS PRINCIPIOS GAUSS-MARKOV</a>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563178b4",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a>\n",
    "#### Importamos los datos del Desafío 1 y las librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ad97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import eval_measures\n",
    "from category_encoders import TargetEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_final')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef69352",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sacamos 2 nulos que nos habían quedado en descr clean\n",
    "data.loc[data.description_Clean.isna(),\"description_Clean\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644b409",
   "metadata": {},
   "source": [
    "<a id=\"section_variables\"></a>\n",
    "#### Genramos nuevas variables para obtener mejores predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565eccf",
   "metadata": {},
   "source": [
    "##### Nuevas variables dicotómicas con las amenites que encontramos en description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities = ['balcon','toilette','suite', 'jardin', 'patio', \n",
    "             'dependencia', 'pileta', 'vigilancia', 'parrilla', 'ascensor', 'gimnasio', 'quincho', 'sauna', 'solarium', 'sala de juegos', \n",
    "             'laundry', 'sum', 'terraza', 'cochera', 'garage']\n",
    "\n",
    "for i in amenities:\n",
    "    data[i]= (data.description_Clean.str.contains(i)).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2b5fd",
   "metadata": {},
   "source": [
    "##### Agregamos una columna que tenga el precio promedio por Property Type y state_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63365d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una serie que tenga la media de precio por 'property_type' y 'state_name'\n",
    "grouped_by_price = data.groupby(['property_type','state_name', 'Barrio_1', 'Barrio_2' ])['price_clean'].transform('median').round(0)\n",
    "\n",
    "data['precio_promedio'] = grouped_by_price\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a77360",
   "metadata": {},
   "source": [
    "##### Regex para ver la cantidad de baños"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraccion de n de baños de la descripcion\n",
    "banos_extract = data['description_Clean'].str.extract(\"([\\d])(\\s+)?(banos|bano|baño|baños)\", re.IGNORECASE)\n",
    "\n",
    "data[\"banos_Clean\"] = banos_extract[0].str.replace(\",\",\"\").replace(\".\",\"\")\n",
    "\n",
    "#eliminamos valores nulos\n",
    "data[\"banos_Clean\"] = data[\"banos_Clean\"].apply(lambda x: x if x is not np.NaN else 0)\n",
    "\n",
    "#convertimos a integer\n",
    "data[\"banos_Clean\"]=pd.to_numeric(data[\"banos_Clean\"], errors=\"coerce\", downcast=\"integer\")\n",
    "\n",
    "#reemplazamos donde banos son 0 y suponemos que tienen por lo menos 1\n",
    "data.loc[data[\"banos_Clean\"]==0,\"banos_Clean\"] = 1\n",
    "\n",
    "#reemplazamos donde figura la palabra \"banos\" y solo figura 1, suponemos que tienen por lo menos 2\n",
    "data.loc[(data[\"banos_Clean\"]==1)&(data[\"description_Clean\"].str.contains(\"banos\")),\"banos_Clean\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d55923",
   "metadata": {},
   "source": [
    "##### Generación de Target Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usamos target encoder para Barrio_1 / Barrio_2 / Barrio_3 / State_name\n",
    "\n",
    "for col in ['state_name', 'Barrio_1', 'Barrio_2']:\n",
    "  encoder = TargetEncoder()\n",
    "  data[col+\"_Target\"] = encoder.fit_transform(data[col], data['surface_covered_in_m2_Clean'])\n",
    "    \n",
    "   #For the case of continuous target: features are replaced with a blend of the expected value of the target given particular \n",
    "#categorical value and the expected value of the target over all the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9296f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371910c6",
   "metadata": {},
   "source": [
    "##### Dropeamos las columnas que no vamos a meter dentro del modelo, ya sea por el valor que aportan o para no tener data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"Unnamed: 0\",\"description_Clean\", 'price_clean', 'Barrio_3'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6d872",
   "metadata": {},
   "source": [
    "<a id=\"section_dummies\"></a>\n",
    "#### Generación de Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una lista con las variables categoricas a transformar en dummies\n",
    "\n",
    "categoricals = ['property_type', 'state_name', 'Barrio_1', 'Barrio_2']\n",
    "\n",
    "#generamos un dataframe sólo con las categoricals\n",
    "\n",
    "X_cat = data[categoricals]\n",
    "\n",
    "#instanciamos el OneHotEncoder\n",
    "#no usamos el drop = First porque en el paso siguiente dropeamos arbitrariamente distintas dummies, por lo que no necesitamos que \n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "#fiteamos con X_cat (esto lo que hace es aprender todas las categorías posibles)\n",
    "\n",
    "enc.fit(X_cat)\n",
    "\n",
    "#listamos las categorías\n",
    "enc.categories_\n",
    "\n",
    "\n",
    "#generamos el dataframe de dummies\n",
    "\n",
    "dummies = enc.transform(X_cat).toarray()\n",
    "dummies_df = pd.DataFrame(dummies)\n",
    "\n",
    "#Generamos los nombres de las columnas\n",
    "\n",
    "col_names = [categoricals[i] + '_' + enc.categories_[i] for i in range(len(categoricals)) ]\n",
    "\n",
    "#dropeamos los primeros nombres de las columnas, porque es lo que hicimos cuando instanciamos el OneHotEncoder\n",
    "\n",
    "#col_names_drop_first = [sublist[i] for sublist in col_names for i in range(len(sublist)) if i != 0]\n",
    "\n",
    "col_names_sin_drop = [sublist[i] for sublist in col_names for i in range(len(sublist))]\n",
    "\n",
    "\n",
    "#renombramos las columnas en el dataframe de dummies\n",
    "\n",
    "#dummies_df.columns = col_names_drop_first\n",
    "dummies_df.columns = col_names_sin_drop\n",
    "\n",
    "\n",
    "dummies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropeamos arbitrariamente las dummies que tienen valores True por menos de 500 registros. Entendemos que si menos del 0.5%\n",
    "# de los datos cuentan con esa característica, no nos proporciona info útil\n",
    "\n",
    "columnas_dummies = dummies_df.columns\n",
    "for i in columnas_dummies:\n",
    "    if dummies_df[i].sum() < 500:\n",
    "        dummies_df.drop(i, axis = 1, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227141a",
   "metadata": {},
   "source": [
    "<a id=\"section_estandar\"></a>\n",
    "#### Estandarizamos las variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#listamos todas las variables numericas\n",
    "numericas = ['surface_total_in_m2_Clean', 'surface_covered_in_m2_Clean', 'precio_promedio','rooms_Clean', 'banos_Clean', 'state_name_Target','Barrio_1_Target', 'Barrio_2_Target']\n",
    "\n",
    "#instanciamos el scaler\n",
    "\n",
    "#scaler_std = preprocessing.StandardScaler()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#generamos el dataframe a estandarizar\n",
    "\n",
    "X_num = data[numericas]\n",
    "\n",
    "#fiteamos con el scaler\n",
    "\n",
    "min_max_scaler.fit(X_num)\n",
    "\n",
    "#transoformamos y generamos el data frame con los valores estandarizados\n",
    "\n",
    "X_scal = min_max_scaler.transform(X_num)\n",
    "X_scal_df = pd.DataFrame(X_scal)\n",
    "\n",
    "#renombramos las columnas del DF con el agregado de 'std' para identificarlas\n",
    "X_scal_df.columns = [i + '_std' for i in numericas]\n",
    "X_scal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a67bcd",
   "metadata": {},
   "source": [
    "<a id=\"section_data\"></a>\n",
    "#### Preparamos el dataframe que va a entrar al modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6fba4c",
   "metadata": {},
   "source": [
    "##### Unimos data, dummies y normalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c026d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unimos data a dummies_df, X_scal_df y dropeamos lo que no necesitamos mas en data \n",
    "\n",
    "data = pd.concat([data, X_scal_df, dummies_df], axis = 1)\n",
    "\n",
    "data.drop(['property_type', 'surface_total_in_m2_Clean', 'surface_covered_in_m2_Clean', \n",
    "          'precio_promedio', 'state_name', 'Barrio_1', 'Barrio_2', 'rooms_Clean', 'banos_Clean'], axis=1, inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018dede",
   "metadata": {},
   "source": [
    "##### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_feature = ['balcon', 'toilette', 'suite', 'jardin',\n",
    "       'patio', 'dependencia', 'pileta', 'vigilancia', 'parrilla', 'ascensor',\n",
    "       'gimnasio', 'quincho', 'sauna', 'solarium', 'sala de juegos', 'laundry',\n",
    "       'sum', 'terraza', 'cochera', 'garage', 'surface_total_in_m2_Clean_std',\n",
    "       'surface_covered_in_m2_Clean_std', 'precio_promedio_std',\n",
    "       'rooms_Clean_std', 'banos_Clean_std', 'property_type_PH',\n",
    "       'property_type_apartment', 'property_type_house', 'property_type_store',\n",
    "       'state_name_Bs.As. Costa Atlántica',\n",
    "       'state_name_Bs.As. G.B.A. Zona Norte',\n",
    "       'state_name_Bs.As. G.B.A. Zona Oeste',\n",
    "       'state_name_Bs.As. G.B.A. Zona Sur', 'state_name_Bs.As. Interior',\n",
    "       'state_name_Capital Federal', 'state_name_Córdoba',\n",
    "       'state_name_Mendoza', 'state_name_Neuquén', 'state_name_Río Negro',\n",
    "       'state_name_Santa Fe', 'state_name_Tucumán', 'Barrio_1_Almagro',\n",
    "       'Barrio_1_Almirante Brown', 'Barrio_1_Avellaneda', 'Barrio_1_Balvanera',\n",
    "       'Barrio_1_Barrio Norte', 'Barrio_1_Belgrano', 'Barrio_1_Caballito',\n",
    "       'Barrio_1_Córdoba', 'Barrio_1_Escobar', 'Barrio_1_Flores',\n",
    "       'Barrio_1_General San Martín', 'Barrio_1_Ituzaingó',\n",
    "       'Barrio_1_La Matanza', 'Barrio_1_La Plata', 'Barrio_1_Lanús',\n",
    "       'Barrio_1_Lomas de Zamora', 'Barrio_1_Mar del Plata', 'Barrio_1_Morón',\n",
    "       'Barrio_1_Nuñez', 'Barrio_1_Palermo', 'Barrio_1_Pilar',\n",
    "       'Barrio_1_Pinamar', 'Barrio_1_Quilmes', 'Barrio_1_Recoleta',\n",
    "       'Barrio_1_Rosario', 'Barrio_1_San Fernando', 'Barrio_1_San Isidro',\n",
    "       'Barrio_1_San Miguel', 'Barrio_1_San Telmo', 'Barrio_1_Santa Fe',\n",
    "       'Barrio_1_Sin definir', 'Barrio_1_Tigre', 'Barrio_1_Tres de Febrero',\n",
    "       'Barrio_1_Vicente López', 'Barrio_1_Villa Crespo',\n",
    "       'Barrio_1_Villa Urquiza', 'Barrio_2_Adrogué', 'Barrio_2_Banfield',\n",
    "       'Barrio_2_Castelar', 'Barrio_2_La Plata', 'Barrio_2_Lanús',\n",
    "       'Barrio_2_Lomas de Zamora', 'Barrio_2_Martínez', 'Barrio_2_Morón',\n",
    "       'Barrio_2_Nordelta', 'Barrio_2_Olivos', 'Barrio_2_Ramos Mejía',\n",
    "       'Barrio_2_Sin definir', 'Barrio_2_Temperley', 'Barrio_2_Tigre',\n",
    "       'Barrio_2_Villa Ballester', 'state_name_Target','Barrio_1_Target', 'Barrio_2_Target']\n",
    "\n",
    "col_target = \"price_usd_per_m2_Clean\"\n",
    "\n",
    "X = data[col_feature]\n",
    "\n",
    "y = data[col_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86264d",
   "metadata": {},
   "source": [
    "<a id=\"section_reg_sts\"></a>\n",
    "#### LinearRegression con StatsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de173a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sumamos la constante a las features de train y test \n",
    "\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "\n",
    "#instanciamos y fiteamos el modelo en el mismo paso\n",
    "\n",
    "linereg_sm = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "#Vemos el summary\n",
    "\n",
    "linereg_sm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vemos las predecimos y evaluamos en train \n",
    "\n",
    "sm_prediction_train = linereg_sm.predict(X_train_sm)\n",
    "print(eval_measures.rmse(y_train, sm_prediction_train))\n",
    "\n",
    "\n",
    "sm_prediction_test = linereg_sm.predict(X_test_sm)\n",
    "print(eval_measures.rmse(y_test, sm_prediction_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a452a1",
   "metadata": {},
   "source": [
    "<a id=\"section_reg_skl\"></a>\n",
    "#### LinearRegression Multiple con Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regLinMul_function(dataset, col_feature, col_target):\n",
    " \n",
    "    reg = linear_model.LinearRegression()\n",
    "\n",
    "    # Entrenamos el modelo \n",
    "\n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    #realizamos predicciones con el set de testeo\n",
    "    y_pred= reg.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #vemos los Betas\n",
    "\n",
    "    # print (reg.intercept_)\n",
    "    # print (reg.coef_)\n",
    "\n",
    "    # print(list(zip(X.columns, reg.coef_)))\n",
    "\n",
    "    #evaluamos el r2 en Train \n",
    "\n",
    "    print ('R2 en Train:', reg.score(X_train, y_train))\n",
    "    \n",
    "    # evaluamos el r2 en Train ajustado por la cantidad de datos y la cantidad de features\n",
    "    r2_train = reg.score(X_train, y_train)\n",
    "    n = data.shape[0]\n",
    "    p = len(col_feature)\n",
    "    #evaluamos el r2 en Test\n",
    "    print ('R2 en Test:', reg.score(X_test, y_test))\n",
    "    print ('MAE:', metrics.mean_absolute_error(y_test,y_pred))\n",
    "    print ('MSE:', metrics.mean_squared_error(y_test,y_pred))\n",
    "regLin = regLinMul_function(data,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753188e",
   "metadata": {},
   "source": [
    "<a id=\"section_vif\"></a>\n",
    "#### Evaluamos el VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e32042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = data[col_feature]\n",
    "y = data[col_target]\n",
    "\n",
    "\n",
    "series_vif =  pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns)\n",
    "\n",
    "series_vif.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db428195",
   "metadata": {},
   "source": [
    "<a id=\"section_lasso_cv\"></a>\n",
    "#### Lasso con Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciamos el modelo y le metemos varios alphas para que pruebe \n",
    "\n",
    "lassoCV = linear_model.LassoCV(alphas = np.arange(0.001, 1, 0.005), cv = 3, normalize = False)\n",
    "\n",
    "#fiteamos con train\n",
    "\n",
    "lasso_cv_model = lassoCV.fit(X_train, y_train)\n",
    "\n",
    "#el mejor alpha que encontró con Cross Validation \n",
    "\n",
    "print('Best alpha', lasso_cv_model.alpha_)\n",
    "best_alpha_lasso = lasso_cv_model.alpha_\n",
    "\n",
    "#Vemos Beta 0 y los coeficientes \n",
    "\n",
    "#print(lasso_cv_model.intercept_)\n",
    "#print(lasso_cv_model.coef_)\n",
    "\n",
    "#realizamos predicciones con el set de testeo\n",
    "y_pred= lassoCV.predict(X_test)\n",
    "\n",
    "#Observamos las metricas\n",
    "print(\"R2 Train\",lasso_cv_model.score(X_train, y_train))\n",
    "print('R2 Test:', lasso_cv_model.score(X_test, y_test))\n",
    "print('MAE:', metrics.mean_absolute_error(y_test,y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3099f",
   "metadata": {},
   "source": [
    "<a id=\"section_ridge_cv\"></a>\n",
    "#### Ridge con Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciamos el modelo y le metemos varios alphas para que pruebe \n",
    "\n",
    "ridgeCV = linear_model.RidgeCV(alphas = np.arange(1, 5, 0.01), normalize = False)\n",
    "\n",
    "#fiteamos con train\n",
    "\n",
    "ridge_cv_model = ridgeCV.fit(X_train, y_train)\n",
    "\n",
    "#el mejor alpha que encontró con Cross Validation \n",
    "\n",
    "print('Best alpha', ridge_cv_model.alpha_)\n",
    "#best_alpha_ridge = ridge_cv_model.alpha_\n",
    "\n",
    "#Vemos Beta 0 y los coeficientes \n",
    "\n",
    "#ridge_cv_model.intercept_\n",
    "#ridge_cv_model.coef_\n",
    "\n",
    "#realizamos predicciones con el set de testeo\n",
    "y_pred= ridge_cv_model.predict(X_test)\n",
    "\n",
    "#Observamos las metricas\n",
    "print(\"R2 Train\",ridge_cv_model.score(X_train, y_train))\n",
    "print('R2 Test:', ridge_cv_model.score(X_test, y_test))\n",
    "print('MAE:', metrics.mean_absolute_error(y_test,y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52049a3",
   "metadata": {},
   "source": [
    "<a id=\"section_dec_tree\"></a>\n",
    "#### Probamos con DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "reg_tree = tree.DecisionTreeRegressor(random_state=1234)\n",
    "reg_tree.fit(X_train,y_train)\n",
    "#Observamos features mas importantes\n",
    "sorted(list(zip(X.columns,reg_tree.feature_importances_)),key=lambda x: x[1], reverse=True)[:5]\n",
    "#realizamos predicciones con el set de testeo\n",
    "y_pred= reg_tree.predict(X_test)\n",
    "\n",
    "#Observamos las metricas\n",
    "print(\"R2 Train\",reg_tree.score(X_train, y_train))\n",
    "print('R2 Test:', reg_tree.score(X_test, y_test))\n",
    "print('MAE:', metrics.mean_absolute_error(y_test,y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3124e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"section_ensemble\"></a>\n",
    "### Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b105d8c",
   "metadata": {},
   "source": [
    "##### HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32193e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "HGBreg=HistGradientBoostingRegressor(random_state=1234)\n",
    "HGBreg.fit(X_train,y_train)\n",
    "\n",
    "#realizamos predicciones con el set de testeo\n",
    "y_pred= HGBreg.predict(X_test)\n",
    "\n",
    "#Observamos las metricas\n",
    "print(\"R2 Train\",HGBreg.score(X_train, y_train))\n",
    "print('R2 Test:', HGBreg.score(X_test, y_test))\n",
    "print('MAE:', metrics.mean_absolute_error(y_test,y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32a812",
   "metadata": {},
   "source": [
    "##### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFreg=RandomForestRegressor(random_state=1234,n_estimators=100,max_depth=10)\n",
    "RFreg.fit(X_train,y_train) \n",
    "\n",
    "#realizamos predicciones con el set de testeo\n",
    "y_pred= RFreg.predict(X_test)\n",
    "\n",
    "#Observamos las metricas\n",
    "print(\"R2 Train\",RFreg.score(X_train, y_train))\n",
    "print('R2 Test:', RFreg.score(X_test, y_test))\n",
    "print('MAE:', metrics.mean_absolute_error(y_test,y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff9c8a",
   "metadata": {},
   "source": [
    "<a id=\"section_gauss\"></a>\n",
    "\n",
    "#### Supuestos de GAUSS MARKOV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee18f7",
   "metadata": {},
   "source": [
    "##### Linealidad del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ce120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format ='retina'\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "def linearity_test(model, y):\n",
    "    '''\n",
    "    funcion para visualizar e identificar supuestos de linealidad sobre la regression lineal\n",
    "    Args:\n",
    "    * model - fitted OLS model from statsmodels\n",
    "    * y - observed values\n",
    "    '''\n",
    "    fitted_vals = model.predict()\n",
    "    resids = model.resid\n",
    "\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "    ax[0].set_title('Observados vs. Valores Predichos', fontsize=16)\n",
    "    ax[0].set(xlabel='Predichos', ylabel='Observados')\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "    ax[1].set_title('Residos vs. Valores Predichos', fontsize=16)\n",
    "    ax[1].set(xlabel='Predichos', ylabel='Residuos')\n",
    "    \n",
    "linearity_test(linereg_sm, y_train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b88caf",
   "metadata": {},
   "source": [
    "Observar que el patrón inclinado en la grafica de los residuos puede deberse a que el modelo contiene muchas características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ea6bc",
   "metadata": {},
   "source": [
    "##### Esperanza de los residuos igual a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fc184",
   "metadata": {},
   "outputs": [],
   "source": [
    "linereg_sm.resid.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b7f324",
   "metadata": {},
   "source": [
    "##### Multicolinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X_train_sm.values, i) for i in range(X_train_sm.shape[1])]\n",
    "pd.DataFrame({'vif': vif[1:]}, index=X.columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a6a8a",
   "metadata": {},
   "source": [
    "##### Homocedasticidad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "def homoscedasticity_test(model):\n",
    "    '''\n",
    "    Function for testing the homoscedasticity of residuals in a linear regression model.\n",
    "    It plots residuals and standardized residuals vs. fitted values and runs Breusch-Pagan and Goldfeld-Quandt tests.\n",
    "    \n",
    "    Args:\n",
    "    * model - fitted OLS model from statsmodels\n",
    "    '''\n",
    "    import numpy as np\n",
    "    fitted_vals = model.predict()\n",
    "    resids = model.resid\n",
    "    resids_standardized = model.get_influence().resid_studentized_internal\n",
    "\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "    ax[0].set_title('Residuals vs Fitted', fontsize=16)\n",
    "    ax[0].set(xlabel='Fitted Values', ylabel='Residuals')\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "    ax[1].set_title('Scale-Location', fontsize=16)\n",
    "    ax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')\n",
    "\n",
    "    bp_test = pd.DataFrame(sms.het_breuschpagan(resids, model.model.exog), \n",
    "                           columns=['value'],\n",
    "                           index=['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value'])\n",
    "\n",
    "    gq_test = pd.DataFrame(sms.het_goldfeldquandt(resids, model.model.exog)[:-1],\n",
    "                           columns=['value'],\n",
    "                           index=['F statistic', 'p-value'])\n",
    "\n",
    "    print('\\n Breusch-Pagan test ----')\n",
    "    print(bp_test)\n",
    "    print('\\n Goldfeld-Quandt test ----')\n",
    "    print(gq_test)\n",
    "    print('\\n Residuals plots ----')\n",
    "\n",
    "homoscedasticity_test(linereg_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c621c",
   "metadata": {},
   "source": [
    "##### Autocorrelación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903091ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "acf = smt.graphics.plot_acf(linereg_sm.resid, lags=40, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280de291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def normality_of_residuals_test(model):\n",
    "    '''\n",
    "    Function for drawing the normal QQ-plot of the residuals and running 4 statistical tests to \n",
    "    investigate the normality of residuals.\n",
    "    \n",
    "    Arg:\n",
    "    * model - fitted OLS models from statsmodels\n",
    "    '''\n",
    "    sm.ProbPlot(model.resid).qqplot(line='s');\n",
    "    plt.title('Q-Q plot');\n",
    "\n",
    "    jb = stats.jarque_bera(model.resid)\n",
    "    sw = stats.shapiro(model.resid)\n",
    "    ad = stats.anderson(model.resid, dist='norm')\n",
    "    ks = stats.kstest(model.resid, 'norm')\n",
    "    \n",
    "    print(f'Jarque-Bera test ---- statistic: {jb[0]:.4f}, p-value: {jb[1]}')\n",
    "    print(f'Shapiro-Wilk test ---- statistic: {sw[0]:.4f}, p-value: {sw[1]:.4f}')\n",
    "    print(f'Kolmogorov-Smirnov test ---- statistic: {ks.statistic:.4f}, p-value: {ks.pvalue:.4f}')\n",
    "    print(f'Anderson-Darling test ---- statistic: {ad.statistic:.4f}, 5% critical value: {ad.critical_values[2]:.4f}')\n",
    "    print('If the returned AD statistic is larger than the critical value, then for the 5% significance level, the null hypothesis that the data come from the Normal distribution should be rejected. ')\n",
    "    \n",
    "normality_of_residuals_test(linereg_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c21b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd65a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
